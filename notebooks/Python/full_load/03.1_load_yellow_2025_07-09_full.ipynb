{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cdc1cf",
   "metadata": {},
   "source": [
    "### Load\n",
    "Idempotent monthly load: Before importing a given month, any existing rows for that source file are removed using DELETE WHERE source_file = .... This makes the load repeatable and extensible at any time without creating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcfd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('patrickpaubandt', 'nf_da_onl_13102025', 'public')\n",
      "Setup OK. Schema: s_patrickpaubandt\n"
     ]
    }
   ],
   "source": [
    "## DB SetUp and SetUp Check\n",
    "## For additional help, check the .env.example file.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS_RAW = os.getenv(\"POSTGRES_PASS\")\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "PG_DB   = os.getenv(\"POSTGRES_DB\")\n",
    "PG_SCHEMA = os.getenv(\"POSTGRES_SCHEMA\", \"public\")\n",
    "\n",
    "missing = [k for k,v in {\n",
    "    \"POSTGRES_USER\": PG_USER,\n",
    "    \"POSTGRES_PASS\": PG_PASS_RAW,\n",
    "    \"POSTGRES_HOST\": PG_HOST,\n",
    "    \"POSTGRES_DB\": PG_DB,\n",
    "    \"POSTGRES_SCHEMA\": PG_SCHEMA\n",
    "}.items() if not v]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing env vars: {missing}\")\n",
    "\n",
    "PG_PASS = quote_plus(PG_PASS_RAW)\n",
    "url = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(url, future=True)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(conn.execute(text(\"SELECT current_user, current_database(), current_schema();\")).fetchone())\n",
    "\n",
    "print(\"Setup OK. Schema:\", PG_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ecd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "MONTHS = [\"2025-07\", \"2025-08\", \"2025-09\"]  # expanded\n",
    "\n",
    "def file_name(month: str) -> str:\n",
    "    return f\"yellow_tripdata_{month}.parquet\"\n",
    "\n",
    "def file_url(month: str) -> str:\n",
    "    return f\"{BASE_URL}/{file_name(month)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3a266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted existing rows for yellow_tripdata_2025-07.parquet: 0\n",
      "Will load from: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-07.parquet\n",
      "File already exists: C:\\Users\\patri\\Documents\\Data Analytics - neuefische\\Capstone-project-NYCTaxi\\data\\yellow_tripdata_2025-07.parquet\n",
      "Rows in file: 3898963\n",
      "Row groups: 4\n",
      "Batch 1: inserted 159,633 rows | total inserted: 159,633\n",
      "Batch 2: inserted 149,251 rows | total inserted: 308,884\n",
      "Batch 3: inserted 157,401 rows | total inserted: 466,285\n",
      "Batch 4: inserted 161,510 rows | total inserted: 627,795\n",
      "Batch 5: inserted 158,678 rows | total inserted: 786,473\n",
      "Batch 6: inserted 158,150 rows | total inserted: 944,623\n",
      "Batch 7: inserted 162,202 rows | total inserted: 1,106,825\n",
      "Batch 8: inserted 161,667 rows | total inserted: 1,268,492\n",
      "Batch 9: inserted 157,682 rows | total inserted: 1,426,174\n",
      "Batch 10: inserted 160,038 rows | total inserted: 1,586,212\n",
      "Batch 11: inserted 162,025 rows | total inserted: 1,748,237\n",
      "Batch 12: inserted 157,687 rows | total inserted: 1,905,924\n",
      "Batch 13: inserted 159,030 rows | total inserted: 2,064,954\n",
      "Batch 14: inserted 163,755 rows | total inserted: 2,228,709\n",
      "Batch 15: inserted 49,293 rows | total inserted: 2,278,002\n",
      "Batch 16: inserted 0 rows | total inserted: 2,278,002\n",
      "Batch 17: inserted 0 rows | total inserted: 2,278,002\n",
      "Batch 18: inserted 0 rows | total inserted: 2,278,002\n",
      "Batch 19: inserted 0 rows | total inserted: 2,278,002\n",
      "Batch 20: inserted 0 rows | total inserted: 2,278,002\n",
      "DONE. Total inserted: 2278002\n",
      "AUDIT: {'read_rows': 3898963, 'kept_rows': 2278002, 'dropped_payment_not_1': 1555295, 'dropped_trip_distance_le_0_or_null': 19199, 'dropped_fare_amount_le_0_or_null': 33, 'dropped_total_amount_le_0_or_null': 0, 'dropped_dropoff_not_after_pickup_or_null': 46434, 'dropped_tip_amount_lt_0': 0}\n",
      "\n",
      "Deleted existing rows for yellow_tripdata_2025-08.parquet: 0\n",
      "Will load from: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-08.parquet\n",
      "Downloading to: C:\\Users\\patri\\Documents\\Data Analytics - neuefische\\Capstone-project-NYCTaxi\\data\\yellow_tripdata_2025-08.parquet\n",
      "Rows in file: 3574091\n",
      "Row groups: 4\n",
      "Batch 1: inserted 157,066 rows | total inserted: 157,066\n",
      "Batch 2: inserted 158,608 rows | total inserted: 315,674\n",
      "Batch 3: inserted 161,614 rows | total inserted: 477,288\n",
      "Batch 4: inserted 157,706 rows | total inserted: 634,994\n",
      "Batch 5: inserted 157,217 rows | total inserted: 792,211\n",
      "Batch 6: inserted 161,350 rows | total inserted: 953,561\n",
      "Batch 7: inserted 157,889 rows | total inserted: 1,111,450\n",
      "Batch 8: inserted 156,478 rows | total inserted: 1,267,928\n",
      "Batch 9: inserted 160,456 rows | total inserted: 1,428,384\n",
      "Batch 10: inserted 157,001 rows | total inserted: 1,585,385\n",
      "Batch 11: inserted 156,626 rows | total inserted: 1,742,011\n",
      "Batch 12: inserted 158,887 rows | total inserted: 1,900,898\n",
      "Batch 13: inserted 155,273 rows | total inserted: 2,056,171\n",
      "Batch 14: inserted 67,323 rows | total inserted: 2,123,494\n",
      "Batch 15: inserted 0 rows | total inserted: 2,123,494\n",
      "Batch 16: inserted 0 rows | total inserted: 2,123,494\n",
      "Batch 17: inserted 0 rows | total inserted: 2,123,494\n",
      "Batch 18: inserted 0 rows | total inserted: 2,123,494\n",
      "DONE. Total inserted: 2123494\n",
      "AUDIT: {'read_rows': 3574091, 'kept_rows': 2123494, 'dropped_payment_not_1': 1391353, 'dropped_trip_distance_le_0_or_null': 20385, 'dropped_fare_amount_le_0_or_null': 39, 'dropped_total_amount_le_0_or_null': 0, 'dropped_dropoff_not_after_pickup_or_null': 38820, 'dropped_tip_amount_lt_0': 0}\n",
      "\n",
      "Deleted existing rows for yellow_tripdata_2025-09.parquet: 4251015\n",
      "Will load from: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-09.parquet\n",
      "File already exists: C:\\Users\\patri\\Documents\\Data Analytics - neuefische\\Capstone-project-NYCTaxi\\data\\yellow_tripdata_2025-09.parquet\n",
      "Rows in file: 4251015\n",
      "Row groups: 5\n",
      "Batch 1: inserted 159,580 rows | total inserted: 159,580\n",
      "Batch 2: inserted 164,188 rows | total inserted: 323,768\n",
      "Batch 3: inserted 162,241 rows | total inserted: 486,009\n",
      "Batch 4: inserted 161,445 rows | total inserted: 647,454\n",
      "Batch 5: inserted 166,131 rows | total inserted: 813,585\n",
      "Batch 6: inserted 164,725 rows | total inserted: 978,310\n",
      "Batch 7: inserted 163,001 rows | total inserted: 1,141,311\n",
      "Batch 8: inserted 162,923 rows | total inserted: 1,304,234\n",
      "Batch 9: inserted 167,143 rows | total inserted: 1,471,377\n",
      "Batch 10: inserted 165,879 rows | total inserted: 1,637,256\n",
      "Batch 11: inserted 163,733 rows | total inserted: 1,800,989\n",
      "Batch 12: inserted 162,602 rows | total inserted: 1,963,591\n",
      "Batch 13: inserted 165,639 rows | total inserted: 2,129,230\n",
      "Batch 14: inserted 164,796 rows | total inserted: 2,294,026\n",
      "Batch 15: inserted 161,007 rows | total inserted: 2,455,033\n",
      "Batch 16: inserted 151,246 rows | total inserted: 2,606,279\n",
      "Batch 17: inserted 0 rows | total inserted: 2,606,279\n",
      "Batch 18: inserted 0 rows | total inserted: 2,606,279\n",
      "Batch 19: inserted 0 rows | total inserted: 2,606,279\n",
      "Batch 20: inserted 0 rows | total inserted: 2,606,279\n",
      "Batch 21: inserted 0 rows | total inserted: 2,606,279\n",
      "Batch 22: inserted 0 rows | total inserted: 2,606,279\n",
      "DONE. Total inserted: 2606279\n",
      "AUDIT: {'read_rows': 4251015, 'kept_rows': 2606279, 'dropped_payment_not_1': 1574710, 'dropped_trip_distance_le_0_or_null': 22172, 'dropped_fare_amount_le_0_or_null': 39, 'dropped_total_amount_le_0_or_null': 0, 'dropped_dropoff_not_after_pickup_or_null': 47815, 'dropped_tip_amount_lt_0': 0}\n"
     ]
    }
   ],
   "source": [
    "# Download + read parquet files\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Save data locally\n",
    "DATA_DIR = r\"C:\\Users\\patri\\Documents\\Data Analytics - neuefische\\Capstone-project-NYCTaxi\\data\"\n",
    "\n",
    "# Target schema (adjusted: without passenger_count, store_and_fwd_flag)\n",
    "TARGET_COLS = [\n",
    "    \"vendorid\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"trip_distance\",\"ratecodeid\",\n",
    "    \"pulocationid\",\"dolocationid\",\"payment_type\",\n",
    "    \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\n",
    "    \"congestion_surcharge\",\"airport_fee\",\"cbd_congestion_fee\",\n",
    "    \"source_file\"\n",
    "]\n",
    "\n",
    "# For the projection when reading (the source_file column is added later in normalize_df)\n",
    "READ_COLS = [c for c in TARGET_COLS if c != \"source_file\"]\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, source_file: str) -> pd.DataFrame:\n",
    "    # Normalize column names\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # Add source_file\n",
    "    df[\"source_file\"] = source_file\n",
    "\n",
    "    # Ensure all target cols exist\n",
    "    for c in TARGET_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    df = df[TARGET_COLS]\n",
    "    return df\n",
    "\n",
    "BATCH_SIZE = 200_000\n",
    "\n",
    "for month in MONTHS:\n",
    "    sf = file_name(month)\n",
    "    url = file_url(month)\n",
    "\n",
    "    # Idempotent: delete any existing rows for the month\n",
    "    with engine.begin() as conn:\n",
    "        deleted = conn.execute(\n",
    "            text(f\"DELETE FROM {PG_SCHEMA}.stg_yellow_trips WHERE source_file = :sf\"),\n",
    "            {\"sf\": sf}\n",
    "        ).rowcount\n",
    "\n",
    "    print(f\"\\nDeleted existing rows for {sf}: {deleted}\")\n",
    "    print(\"Will load from:\", url)\n",
    "\n",
    "    local_path = os.path.join(DATA_DIR, sf)\n",
    "\n",
    "    if not os.path.exists(local_path):\n",
    "        print(\"Downloading to:\", local_path)\n",
    "        with requests.get(url, stream=True, timeout=300) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):  # 1MB\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "    else:\n",
    "        print(\"File already exists:\", local_path)\n",
    "\n",
    "    # read Parquet in batches\n",
    "    pf = pq.ParquetFile(local_path)\n",
    "    print(\"Rows in file:\", pf.metadata.num_rows)\n",
    "    print(\"Row groups:\", pf.num_row_groups)\n",
    "\n",
    "    # --- NEU: Case-safe Projection Liste aus dem Parquet Schema bauen ---\n",
    "    schema_cols = pf.schema.names\n",
    "    col_map = {c.lower(): c for c in schema_cols}  # lowercase -> echter Parquet-Name\n",
    "\n",
    "    READ_COLS_PARQUET = [col_map[c] for c in READ_COLS if c in col_map]\n",
    "\n",
    "    missing = [c for c in READ_COLS if c not in col_map]\n",
    "    if missing:\n",
    "        print(\"WARNING: Diese Columns gibt's im Parquet nicht (werden als NULL gefÃ¼llt):\", missing)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    inserted_total = 0\n",
    "\n",
    "    # Audit counters (neu)\n",
    "    audit = {\n",
    "        \"read_rows\": 0,\n",
    "        \"kept_rows\": 0,\n",
    "        \"dropped_payment_not_1\": 0,\n",
    "        \"dropped_trip_distance_le_0_or_null\": 0,\n",
    "        \"dropped_fare_amount_le_0_or_null\": 0,\n",
    "        \"dropped_total_amount_le_0_or_null\": 0,\n",
    "        \"dropped_dropoff_not_after_pickup_or_null\": 0,\n",
    "        \"dropped_tip_amount_lt_0\": 0,\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(pf.iter_batches(batch_size=BATCH_SIZE, columns=READ_COLS_PARQUET)):\n",
    "        df = batch.to_pandas()\n",
    "        audit[\"read_rows\"] += len(df)\n",
    "\n",
    "        df = normalize_df(df, sf)\n",
    "\n",
    "        # ---- Type normalization for filters ----\n",
    "        df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "        df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "        for c in [\"trip_distance\", \"fare_amount\", \"total_amount\", \"tip_amount\", \"payment_type\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # ---- Import filters----\n",
    "\n",
    "        # payment_type = 1\n",
    "        m = df[\"payment_type\"] == 1\n",
    "        audit[\"dropped_payment_not_1\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        # trip_distance > 0\n",
    "        m = df[\"trip_distance\"].notna() & (df[\"trip_distance\"] > 0)\n",
    "        audit[\"dropped_trip_distance_le_0_or_null\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        # fare_amount > 0\n",
    "        m = df[\"fare_amount\"].notna() & (df[\"fare_amount\"] > 0)\n",
    "        audit[\"dropped_fare_amount_le_0_or_null\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        # total_amount > 0\n",
    "        m = df[\"total_amount\"].notna() & (df[\"total_amount\"] > 0)\n",
    "        audit[\"dropped_total_amount_le_0_or_null\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        # dropoff > pickup (duration > 0)\n",
    "        m = (\n",
    "            df[\"tpep_pickup_datetime\"].notna() &\n",
    "            df[\"tpep_dropoff_datetime\"].notna() &\n",
    "            (df[\"tpep_dropoff_datetime\"] > df[\"tpep_pickup_datetime\"])\n",
    "        )\n",
    "        audit[\"dropped_dropoff_not_after_pickup_or_null\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        # tip_amount >= 0\n",
    "        m = df[\"tip_amount\"].isna() | (df[\"tip_amount\"] >= 0)\n",
    "        audit[\"dropped_tip_amount_lt_0\"] += int((~m).sum())\n",
    "        df = df[m]\n",
    "\n",
    "        audit[\"kept_rows\"] += len(df)\n",
    "\n",
    "        # Guard-rails: stop if location IDs are basically all NULL in this batch\n",
    "        if len(df) > 0:\n",
    "            null_rates = df[[\"pulocationid\", \"dolocationid\"]].isna().mean()\n",
    "            if (null_rates > 0.9).any():\n",
    "                raise ValueError(f\"STOP: Mapping failed in batch {i}. Null rates: {null_rates.to_dict()}\")\n",
    "\n",
    "        # Insert\n",
    "        if len(df) > 0:\n",
    "            df.to_sql(\n",
    "                name=\"stg_yellow_trips\",\n",
    "                con=engine,\n",
    "                schema=PG_SCHEMA,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "                chunksize=10_000\n",
    "            )\n",
    "\n",
    "        inserted_total += len(df)\n",
    "\n",
    "        # Progress print every batch\n",
    "        print(f\"Batch {i+1}: inserted {len(df):,} rows | total inserted: {inserted_total:,}\")\n",
    "\n",
    "    print(\"DONE. Total inserted:\", inserted_total)\n",
    "    print(\"AUDIT:\", audit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows loaded for file: 2278002\n",
      "Date coverage: (datetime.datetime(2009, 1, 1, 8, 52, 26), datetime.datetime(2025, 7, 31, 23, 59, 58), datetime.datetime(2009, 1, 1, 10, 0, 26), datetime.datetime(2025, 8, 1, 22, 24, 4))\n",
      "Critical nulls: (0, 0, 0, 0)\n",
      "Payment counts: [(1, 2278002)]\n"
     ]
    }
   ],
   "source": [
    "## Validation of the Import \n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "sf = \"yellow_tripdata_2025-07.parquet\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # 1) Row count by file\n",
    "    n = conn.execute(text(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).scalar()\n",
    "\n",
    "    # 2) Date coverage\n",
    "    date_cov = conn.execute(text(f\"\"\"\n",
    "        SELECT\n",
    "          MIN(tpep_pickup_datetime) AS min_pickup,\n",
    "          MAX(tpep_pickup_datetime) AS max_pickup,\n",
    "          MIN(tpep_dropoff_datetime) AS min_dropoff,\n",
    "          MAX(tpep_dropoff_datetime) AS max_dropoff\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).fetchone()\n",
    "\n",
    "    # 3) Critical NULLs (should be low for location IDs)\n",
    "    nulls = conn.execute(text(f\"\"\"\n",
    "        SELECT\n",
    "          SUM(CASE WHEN pulocationid IS NULL THEN 1 ELSE 0 END) AS pu_nulls,\n",
    "          SUM(CASE WHEN dolocationid IS NULL THEN 1 ELSE 0 END) AS do_nulls,\n",
    "          SUM(CASE WHEN tpep_pickup_datetime IS NULL THEN 1 ELSE 0 END) AS pickup_nulls,\n",
    "          SUM(CASE WHEN tpep_dropoff_datetime IS NULL THEN 1 ELSE 0 END) AS dropoff_nulls\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).fetchone()\n",
    "\n",
    "    # 4) Quick payment sanity (sollte jetzt nur noch 1 sein)\n",
    "    payment = conn.execute(text(f\"\"\"\n",
    "        SELECT payment_type, COUNT(*) AS n\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "        GROUP BY 1\n",
    "        ORDER BY n DESC\n",
    "    \"\"\"), {\"sf\": sf}).fetchall()\n",
    "\n",
    "print(\"Rows loaded for file:\", n)\n",
    "print(\"Date coverage:\", date_cov)\n",
    "print(\"Critical nulls:\", nulls)\n",
    "print(\"Payment counts:\", payment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
