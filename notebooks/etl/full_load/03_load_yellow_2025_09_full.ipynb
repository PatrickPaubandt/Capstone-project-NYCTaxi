{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cdc1cf",
   "metadata": {},
   "source": [
    "Idempotenter Monats-Load: Vor dem Import eines Monats werden vorhandene Zeilen dieser Quelldatei über DELETE WHERE source_file = ... entfernt. Dadurch kann der Load jederzeit wiederholt oder erweitert werden, ohne Duplikate zu erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcfd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('patrickpaubandt', 'nf_da_onl_13102025', 'public')\n",
      "Setup OK. Schema: s_patrickpaubandt\n"
     ]
    }
   ],
   "source": [
    "## DB SetUp and SetUp Check\n",
    "## For additional help, check the .env.example file.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS_RAW = os.getenv(\"POSTGRES_PASS\")\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "PG_DB   = os.getenv(\"POSTGRES_DB\")\n",
    "PG_SCHEMA = os.getenv(\"POSTGRES_SCHEMA\", \"public\")\n",
    "\n",
    "missing = [k for k,v in {\n",
    "    \"POSTGRES_USER\": PG_USER,\n",
    "    \"POSTGRES_PASS\": PG_PASS_RAW,\n",
    "    \"POSTGRES_HOST\": PG_HOST,\n",
    "    \"POSTGRES_DB\": PG_DB,\n",
    "    \"POSTGRES_SCHEMA\": PG_SCHEMA\n",
    "}.items() if not v]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing env vars: {missing}\")\n",
    "\n",
    "PG_PASS = quote_plus(PG_PASS_RAW)\n",
    "url = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(url, future=True)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(conn.execute(text(\"SELECT current_user, current_database(), current_schema();\")).fetchone())\n",
    "\n",
    "print(\"Setup OK. Schema:\", PG_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ecd555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing rows for yellow_tripdata_2025-09.parquet: 0\n",
      "Will load from: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-09.parquet\n"
     ]
    }
   ],
   "source": [
    "## Configure Month Downloadlink + delete idempotent\n",
    "\n",
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "MONTHS = [\"2025-09\"]  # später einfach erweitern: [\"2025-09\", \"2025-10\", ...]\n",
    "\n",
    "def file_name(month: str) -> str:\n",
    "    return f\"yellow_tripdata_{month}.parquet\"\n",
    "\n",
    "def file_url(month: str) -> str:\n",
    "    return f\"{BASE_URL}/{file_name(month)}\"\n",
    "\n",
    "# Idempotent: vorhandene Zeilen für den Monat löschen\n",
    "month = MONTHS[0]\n",
    "sf = file_name(month)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    deleted = conn.execute(\n",
    "        text(f\"DELETE FROM {PG_SCHEMA}.stg_yellow_trips WHERE source_file = :sf\"),\n",
    "        {\"sf\": sf}\n",
    "    ).rowcount\n",
    "\n",
    "print(f\"Deleted existing rows for {sf}: {deleted}\")\n",
    "print(\"Will load from:\", file_url(month))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3a266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to: yellow_tripdata_2025-09.parquet\n",
      "Rows in file: 4251015\n",
      "Row groups: 5\n",
      "Batch 1: inserted 200,000 rows | total inserted: 200,000\n",
      "Batch 2: inserted 200,000 rows | total inserted: 400,000\n",
      "Batch 3: inserted 200,000 rows | total inserted: 600,000\n",
      "Batch 4: inserted 200,000 rows | total inserted: 800,000\n",
      "Batch 5: inserted 200,000 rows | total inserted: 1,000,000\n",
      "Batch 6: inserted 200,000 rows | total inserted: 1,200,000\n",
      "Batch 7: inserted 200,000 rows | total inserted: 1,400,000\n",
      "Batch 8: inserted 200,000 rows | total inserted: 1,600,000\n",
      "Batch 9: inserted 200,000 rows | total inserted: 1,800,000\n",
      "Batch 10: inserted 200,000 rows | total inserted: 2,000,000\n",
      "Batch 11: inserted 200,000 rows | total inserted: 2,200,000\n",
      "Batch 12: inserted 200,000 rows | total inserted: 2,400,000\n",
      "Batch 13: inserted 200,000 rows | total inserted: 2,600,000\n",
      "Batch 14: inserted 200,000 rows | total inserted: 2,800,000\n",
      "Batch 15: inserted 200,000 rows | total inserted: 3,000,000\n",
      "Batch 16: inserted 200,000 rows | total inserted: 3,200,000\n",
      "Batch 17: inserted 200,000 rows | total inserted: 3,400,000\n",
      "Batch 18: inserted 200,000 rows | total inserted: 3,600,000\n",
      "Batch 19: inserted 200,000 rows | total inserted: 3,800,000\n",
      "Batch 20: inserted 200,000 rows | total inserted: 4,000,000\n",
      "Batch 21: inserted 200,000 rows | total inserted: 4,200,000\n",
      "Batch 22: inserted 51,015 rows | total inserted: 4,251,015\n",
      "DONE. Total inserted: 4251015\n"
     ]
    }
   ],
   "source": [
    "## Download + read parquet files\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import text\n",
    "\n",
    "month = \"2025-09\"\n",
    "sf = file_name(month)\n",
    "url = file_url(month)\n",
    "\n",
    "# Datei lokal zwischenspeichern (im Notebook-Ordner oder in einem temp-Ordner)\n",
    "DATA_DIR = r\"C:\\Users\\patri\\Documents\\Data Analytics - neuefische\\Capstone-project-NYCTaxi\\data\"\n",
    "local_path = os.path.join(DATA_DIR, f\"yellow_tripdata_{month}.parquet\")\n",
    "\n",
    "if not os.path.exists(local_path):\n",
    "    print(\"Downloading to:\", local_path)\n",
    "    with requests.get(url, stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):  # 1MB\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "else:\n",
    "    print(\"File already exists:\", local_path)\n",
    "\n",
    "# Parquet in batches lesen\n",
    "pf = pq.ParquetFile(local_path)\n",
    "print(\"Rows in file:\", pf.metadata.num_rows)\n",
    "print(\"Row groups:\", pf.num_row_groups)\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"vendorid\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"passenger_count\",\"trip_distance\",\"ratecodeid\",\n",
    "    \"store_and_fwd_flag\",\"pulocationid\",\"dolocationid\",\"payment_type\",\n",
    "    \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\n",
    "    \"congestion_surcharge\",\"airport_fee\",\"cbd_congestion_fee\",\n",
    "    \"source_file\"\n",
    "]\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, source_file: str) -> pd.DataFrame:\n",
    "    # Normalize column names\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # Add source_file\n",
    "    df[\"source_file\"] = source_file\n",
    "\n",
    "    # Ensure all target cols exist\n",
    "    for c in TARGET_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    df = df[TARGET_COLS]\n",
    "    return df\n",
    "\n",
    "BATCH_SIZE = 200_000\n",
    "inserted_total = 0\n",
    "\n",
    "for i, batch in enumerate(pf.iter_batches(batch_size=BATCH_SIZE)):\n",
    "    df = batch.to_pandas()\n",
    "    df = normalize_df(df, sf)\n",
    "\n",
    "    # Guard-rails: stop if location IDs are basically all NULL in this batch\n",
    "    null_rates = df[[\"pulocationid\", \"dolocationid\"]].isna().mean()\n",
    "    if (null_rates > 0.9).any():\n",
    "        raise ValueError(f\"STOP: Mapping failed in batch {i}. Null rates: {null_rates.to_dict()}\")\n",
    "\n",
    "    # Insert\n",
    "    df.to_sql(\n",
    "        name=\"stg_yellow_trips\",\n",
    "        con=engine,\n",
    "        schema=PG_SCHEMA,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        method=\"multi\",\n",
    "        chunksize=10_000\n",
    "    )\n",
    "\n",
    "    inserted_total += len(df)\n",
    "\n",
    "    # Progress print every batch\n",
    "    print(f\"Batch {i+1}: inserted {len(df):,} rows | total inserted: {inserted_total:,}\")\n",
    "\n",
    "print(\"DONE. Total inserted:\", inserted_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d5db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows loaded for file: 4251015\n",
      "Date coverage: (datetime.datetime(2025, 8, 31, 23, 45, 38), datetime.datetime(2025, 10, 1, 0, 0, 11), datetime.datetime(2025, 8, 31, 23, 46, 43), datetime.datetime(2025, 10, 3, 14, 49, 31))\n",
      "Critical nulls: (0, 0, 0, 0)\n",
      "Payment counts: [(1, 2676305), (0, 1067195), (2, 372069), (4, 107514), (3, 27932)]\n"
     ]
    }
   ],
   "source": [
    "## Validation of the Import\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "sf = \"yellow_tripdata_2025-09.parquet\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # 1) Row count by file\n",
    "    n = conn.execute(text(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).scalar()\n",
    "\n",
    "    # 2) Date coverage\n",
    "    date_cov = conn.execute(text(f\"\"\"\n",
    "        SELECT\n",
    "          MIN(tpep_pickup_datetime) AS min_pickup,\n",
    "          MAX(tpep_pickup_datetime) AS max_pickup,\n",
    "          MIN(tpep_dropoff_datetime) AS min_dropoff,\n",
    "          MAX(tpep_dropoff_datetime) AS max_dropoff\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).fetchone()\n",
    "\n",
    "    # 3) Critical NULLs (should be low for location IDs)\n",
    "    nulls = conn.execute(text(f\"\"\"\n",
    "        SELECT\n",
    "          SUM(CASE WHEN pulocationid IS NULL THEN 1 ELSE 0 END) AS pu_nulls,\n",
    "          SUM(CASE WHEN dolocationid IS NULL THEN 1 ELSE 0 END) AS do_nulls,\n",
    "          SUM(CASE WHEN tpep_pickup_datetime IS NULL THEN 1 ELSE 0 END) AS pickup_nulls,\n",
    "          SUM(CASE WHEN tpep_dropoff_datetime IS NULL THEN 1 ELSE 0 END) AS dropoff_nulls\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "    \"\"\"), {\"sf\": sf}).fetchone()\n",
    "\n",
    "    # 4) Quick payment sanity\n",
    "    payment = conn.execute(text(f\"\"\"\n",
    "        SELECT payment_type, COUNT(*) AS n\n",
    "        FROM {PG_SCHEMA}.stg_yellow_trips\n",
    "        WHERE source_file = :sf\n",
    "        GROUP BY 1\n",
    "        ORDER BY n DESC\n",
    "    \"\"\"), {\"sf\": sf}).fetchall()\n",
    "\n",
    "print(\"Rows loaded for file:\", n)\n",
    "print(\"Date coverage:\", date_cov)\n",
    "print(\"Critical nulls:\", nulls)\n",
    "print(\"Payment counts:\", payment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
